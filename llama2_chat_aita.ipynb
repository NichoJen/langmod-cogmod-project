{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NichoJen/langmod-cogmod-project.git"
      ],
      "metadata": {
        "id": "znnY9rUNZflM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert the directory\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/langmod-cogmod-project\")"
      ],
      "metadata": {
        "id": "4_7A_aAbbFhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset path\n",
        "aita_dataset_path = \"langmod-cogmod-project/data/aita_dataset_clean.json\""
      ],
      "metadata": {
        "id": "RRJz1fLGcXUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkAucQW7zbxS"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "print(\"Enter llama 2 access token\")\n",
        "access_token = input()\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True, use_auth_token=access_token)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)"
      ],
      "metadata": {
        "id": "C-zM21wfzpQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "llama_2_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "RqOM5Yqk3bYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aita_anlysis_llama2(system_prompt, content, pipeline, do_sample=False):\n",
        "\n",
        "  aita_input = system_prompt(content)\n",
        "\n",
        "  print(aita_input)\n",
        "\n",
        "\n",
        "  sequences = pipeline(\n",
        "        aita_input,\n",
        "        do_sample=do_sample,\n",
        "        top_k=1,\n",
        "        num_return_sequences=1,\n",
        "        max_new_tokens=50,\n",
        "        return_full_text=False,\n",
        "        temperature=.1,\n",
        "        top_p=0.95)\n",
        "\n",
        "  for seq in sequences:\n",
        "        print(f\"Result: {seq['generated_text']}\")\n",
        "\n",
        "  return sequences"
      ],
      "metadata": {
        "id": "RdcMFby83sWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def results_to_json_file(results, path):\n",
        "    with open(path, \"w\", encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=4, sort_keys=True)"
      ],
      "metadata": {
        "id": "-Qp5npUblHTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt for answering with asshole or not asshole\n",
        "system_prompt_aita = lambda x: f\"\"\"\n",
        "        [INST]\n",
        "        <<SYS>>\n",
        "       You are an expert at judging morally complex situations. Your job is to determine whether the person writing a post in an online forum is the asshole or not the asshole.\n",
        "       Analyze the text and determine if the author is in the asshole or not in the given text.\\n\n",
        "      Answer only with your judgement: \"asshole: yes\" or \"asshole: no\" and nothing else.\n",
        "        <</SYS>>\n",
        "        {x}\n",
        "        [/INST]\\n\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "HZOOwO0nnOgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt for answering with yes or no\n",
        "system_prompt_yes_no = lambda x: f\"\"\"\n",
        "        [INST]\n",
        "        <<SYS>>\n",
        "       You are an expert at judging morally complex situations. Your job is to determine whether the person writing a post in an online forum is in the wrong or not.\n",
        "       Analyze the text and determine if the author is in the wrong in the given text.\\n\n",
        "      Do not answer in a complete sentence, and answer only with: \"yes\" or \"no\".\n",
        "        <</SYS>>\n",
        "        {x}\n",
        "        [/INST]\\n\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "V1_0tXgT0IW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(aita_dataset_path) as f:\n",
        "  aita_dataset = json.load(f)"
      ],
      "metadata": {
        "id": "YXDTOqTI6r0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "5cTUOM2f9i9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make yes no results\n",
        "results_yes_no = []\n",
        "outputs_yes_no = defaultdict(int)\n",
        "for example in aita_dataset:\n",
        "  prediction = aita_anlysis_llama2(system_prompt_yes_no, example[\"body\"], llama_2_pipeline, do_sample=True)[0][\"generated_text\"]\n",
        "  outputs_yes_no[prediction] += 1\n",
        "  result = {\"id\": example[\"id\"],\"prediction\": prediction, \"aita_user_verdict\": example[\"verdict\"], \"is_asshole\": example[\"is_asshole\"]}\n",
        "  results_yes_no.append(result)"
      ],
      "metadata": {
        "id": "A-_gxUvj6xyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view outputs for yes no\n",
        "for output, count in outputs_yes_no.items():\n",
        "  print(f\"output: {output}, count: {count}\")"
      ],
      "metadata": {
        "id": "wD52wUiGER4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_to_json_file(results_yes_no, \"results_yes_no.json\")"
      ],
      "metadata": {
        "id": "8gfCq66XkpsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make aita results\n",
        "results_aita = []\n",
        "outputs_aita = defaultdict(int)\n",
        "for example in aita_dataset:\n",
        "  prediction = aita_anlysis_llama2(system_prompt_aita, example[\"body\"], llama_2_pipeline, do_sample=True)[0][\"generated_text\"]\n",
        "  outputs_aita[prediction] += 1\n",
        "  result = {\"id\": example[\"id\"],\"prediction\": prediction, \"aita_user_verdict\": example[\"verdict\"], \"is_asshole\": example[\"is_asshole\"]}\n",
        "  results_aita.append(result)"
      ],
      "metadata": {
        "id": "UVVrOG-gpM6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view outputs for aita\n",
        "for output, count in outputs_aita.items():\n",
        "  print(f\"output: {output}, count: {count}\")"
      ],
      "metadata": {
        "id": "wmyRNq2DqrEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_to_json_file(results_aita, \"results_aita_prompt_1.json\")"
      ],
      "metadata": {
        "id": "amTZ9GKSryWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}